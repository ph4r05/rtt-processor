{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtt_tools import dump_data\n",
    "from rtt_tools.dump_data import *  # pussy died because of this wildcard import\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from graphviz import Digraph, Graph\n",
    "from scipy import stats\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "coloredlogs.CHROOT_FILES = []\n",
    "coloredlogs.install(level=logging.INFO, use_chroot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dump_data.Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-06 11:44:12 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all experiments\n",
      "2019-06-06 11:44:12 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Number of all experiments: 2652\n",
      "2019-06-06 11:44:12 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all batteries, len: 2652\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all tests, len: 18559\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all variants params, len: 1000\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all variant params, len: 2000\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all variants params, len: 1000\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all variant params, len: 2000\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all variants params, len: 1000\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all variant params, len: 2000\n",
      "2019-06-06 11:44:13 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all subtest params, len: 10000\n",
      "2019-06-06 11:44:14 aura.fi.muni.cz rtt_tools.dump_data[7303] INFO Loading all subtest pvalues, len: 10000\n"
     ]
    }
   ],
   "source": [
    "exp_id_list = [10,11,13] #[10,11,12]  # [2,3,4,5,7,8]\n",
    "# loader.load({'no_pvals':True, 'only_pval_cnt': True, 'experiments': exp_id_list})\n",
    "loader.load({'no_pvals':False, 'only_pval_cnt': False, 'experiments': exp_id_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.process()\n",
    "loader.add_passed = True\n",
    "loader.comp_sub_pvals(add_all=True, pick_one=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.experiments[2893].exp_info.meth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sizes -> tests -> test_config -> counts\n",
    "test_configs = collections.defaultdict(\n",
    "    lambda: collections.defaultdict(\n",
    "        lambda: collections.defaultdict(\n",
    "            lambda: 0\n",
    "        )))\n",
    "\n",
    "# Data sizes -> tests -> test_config -> config_data -> counts\n",
    "test_configs_val = collections.defaultdict(\n",
    "    lambda: collections.defaultdict(\n",
    "        lambda: collections.defaultdict(\n",
    "            lambda: collections.defaultdict(\n",
    "                lambda: 0\n",
    "        ))))\n",
    "\n",
    "\n",
    "# data size -> test_flat -> counts\n",
    "flat_configs_types = collections.defaultdict(\n",
    "    lambda: collections.defaultdict(\n",
    "            lambda: 0\n",
    "        ))\n",
    "\n",
    "# data size -> test_flat -> counts\n",
    "flat_configs = collections.defaultdict(\n",
    "    lambda: collections.defaultdict(\n",
    "            lambda: 0\n",
    "        ))\n",
    "\n",
    "# Data sizes -> tests -> test_config+variantval -> counts\n",
    "test_configs_var = collections.defaultdict(\n",
    "    lambda: collections.defaultdict(\n",
    "        lambda: collections.defaultdict(\n",
    "            lambda: 0\n",
    "        )))\n",
    "\n",
    "\n",
    "all_subs = []\n",
    "for tt in loader.tests.values():\n",
    "    exp = tt.battery.exp\n",
    "    size = exp.exp_info.size\n",
    "    tt_id = '|'.join(reversed(tt.short_desc()))\n",
    "\n",
    "    for vv in tt.variants.values():\n",
    "        cfs = '|'.join([str(x) for x in vv.settings.keys_tuple()])\n",
    "        cfv = '|'.join([str(x) for x in vv.settings.values_tuple()])\n",
    "\n",
    "        for ss in vv.sub_tests.values():\n",
    "            tfs = '|'.join([str(x) for x in ss.params.keys_tuple()])\n",
    "            tfv = '|'.join([str(x) for x in ss.params.values_tuple()])\n",
    "            tcfg = '{%s}{%s}' % (cfs, tfs)\n",
    "            tcfg_val = '{%s}{%s}' % (cfv, tfv)\n",
    "            type_val = '[%s][%s]' % (tt_id, tcfg)\n",
    "            var_val = '{%s}{%s}' % (cfs, cfv) \n",
    "            full_val = '[%s][%s][%s]' % (tt_id, tcfg, tcfg_val)\n",
    "\n",
    "            test_configs[size][tt_id][tcfg] += 1\n",
    "            test_configs['ALL'][tt_id][tcfg] += 1\n",
    "            \n",
    "            test_configs_var[size][tt_id][var_val] += 1\n",
    "            test_configs_var['ALL'][tt_id][var_val] += 1\n",
    "            \n",
    "            test_configs_val[size][tt_id][tcfg][tcfg_val] += 1\n",
    "            test_configs_val['ALL'][tt_id][tcfg][tcfg_val] += 1\n",
    "            \n",
    "            flat_configs[size][full_val] += 1\n",
    "            flat_configs['ALL'][full_val] += 1\n",
    "            flat_configs_types[size][type_val] += 1\n",
    "            flat_configs_types['ALL'][type_val] += 1\n",
    "            \n",
    "            if len(ss.pvals) > 0:\n",
    "                all_subs.append(ss)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Broken fragment for pvalues test, SKIP\n",
    "random.shuffle(cursubs)\n",
    "used_pval_len = []\n",
    "ownks = []\n",
    "for st in cursubs: \n",
    "    stat = pick_km_statistic(st.stats)\n",
    "    ksres = stats.kstest(st.pvals, 'uniform')#mode='asymp')\n",
    "    ownks.append(ksres[1])\n",
    "    used_pval_len.append(len(st.pvals))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Broken fragment for pvalues test, SKIP\n",
    "def logdif(a,b):\n",
    "    if a == 0 or b == 0: return None\n",
    "    return math.log(a,2) - math.log(b,2)\n",
    "\n",
    "def get_pval_partition(pval, pvalbins):\n",
    "    binidx = 0\n",
    "    for idx, b in enumerate(pvalbins):\n",
    "        if pval < b:\n",
    "            return idx\n",
    "    return len(pvalbins)\n",
    "\n",
    "errvct = []\n",
    "for ix in range(len(cursubs)):\n",
    "    stat = pick_km_statistic(cursubs[ix].stats)\n",
    "    err = (ownks[ix] - stat.value) #logdif(ownks[ix], stat.value)\n",
    "    if err is None: continue\n",
    "    errvct.append(err)\n",
    "\n",
    "bigerrs = []\n",
    "pvalbins = [0.000001, 0.25, 0.5]  #[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.25, 0.5]\n",
    "pvals_data = collections.defaultdict(lambda: [])\n",
    "for ix, x in enumerate(errvct):\n",
    "    stat = pick_km_statistic(cursubs[ix].stats)\n",
    "    binval = ownks[ix]  # stat.value\n",
    "    pvals_data[get_pval_partition(binval, pvalbins)].append(errvct[ix])\n",
    "    if abs(errvct[ix]) > 0.05 and stat.value > 0.0001 and stat.value < 0.1: #ownks[ix] > 0.001 and ownks[ix] < 0.1:\n",
    "        bigerrs.append(ix)\n",
    "\n",
    "errvctab = [abs(x) for x in errvct]\n",
    "print(len(bigerrs))\n",
    "print('Num data: %s' % len(used_pval_len))\n",
    "print('Err min: %s, max: %s, Avg: %s, stddev: %s' % (min(errvctab), max(errvctab), np.average(errvctab), np.std(errvctab)))\n",
    "print(json.dumps(stat_names_dct, indent=2))\n",
    "\n",
    "print('Pvals min: %s, max: %s, avg: %s, stdev: %s' % (min(used_pval_len), max(used_pval_len), np.average(used_pval_len), np.std(used_pval_len)))\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_xlabel('pval count')\n",
    "# ax.set_ylabel('Counts')\n",
    "# plt.hist(used_pval_len, log=True)\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.hist(errvctab, bins=25, log=True) \n",
    "# ax.set_xlabel('error |RTT_KS_P - KS_P|')\n",
    "# ax.set_ylabel('Counts')\n",
    "# plt.show()\n",
    "\n",
    "num_plots = len(pvals_data)\n",
    "fig, axes = plt.subplots(num_plots+1, 1, sharey=True, sharex=False, figsize=(7,15),  constrained_layout=True)\n",
    "for i in range(num_plots):\n",
    "     axes[i].set_xlabel('error |KS_P - RTT_KS_P|, KS_Pvalbin bin %s' % (pvalbins[i] if i < len(pvalbins) else '>=%s'%pvalbins[-1]))\n",
    "     axes[i].set_ylabel('Counts')\n",
    "     axes[i].hist(pvals_data[i], log=True)\n",
    "axes[-1].hist(errvct, log=True)\n",
    "axes[-1].set_xlabel('error |KS_P - RTT_KS_P|, total')\n",
    "axes[-1].set_ylabel('Counts')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(test_configs_var, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst = sorted(set(list(flat_configs_types['ALL'].keys())))\n",
    "print(json.dumps(lst, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = sorted(set(list(flat_configs['ALL'].keys())))\n",
    "print(json.dumps(lst, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(flat_configs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stables = set()\n",
    "for msize in flat_configs_types:\n",
    "    tests = flat_configs_types[msize]\n",
    "    for t in tests:\n",
    "        stables.add(t)\n",
    "    \n",
    "for msize in flat_configs_types:\n",
    "    tests = flat_configs_types[msize]\n",
    "    for test in tests:\n",
    "        # other sizes\n",
    "        for msize_other in [x for x in flat_configs_types if x != msize]:\n",
    "            if test not in flat_configs_types[msize_other]:\n",
    "                stables.discard(test)\n",
    "                #msize = int(msize)\n",
    "                #msize_other = int(msize_other)\n",
    "                print('DIf: %10s->%10s: %s' % (msize, msize_other, test))\n",
    "    #print(msize)\n",
    "    \n",
    "print('Stables....')\n",
    "for xx in stables:\n",
    "    print('.. %s' % xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test analysis and scoring\n",
    "exps = list(loader.experiments.values())\n",
    "exps.sort(key=lambda x: (x.exp_info.size, x.exp_info.meth, x.exp_info.seed, x.exp_info.fnc_name, x.exp_info.fnc_round))\n",
    "exps_grouper = lambda x: (x.exp_info.size, x.exp_info.meth, x.exp_info.fnc_name, x.exp_info.fnc_round)  # aggregate different runs with different SEEDs\n",
    "rev_exp = {x.name: i for i,x in enumerate(exps)}\n",
    "\n",
    "# Iterate over test\n",
    "# test[name] = [pvals] per experiment, on a fixed position. None if not present.\n",
    "tests = collections.defaultdict(lambda: [None] * len(exps))\n",
    "test_ids_counts = collections.defaultdict(lambda: 0)\n",
    "for tt in loader.tests.values():\n",
    "    tt_id = '|'.join(reversed(tt.short_desc()))\n",
    "    tests[tt_id][rev_exp[tt.battery.exp.name]] = tt.get_single_pval()\n",
    "    # print(tt.get_single_pval(), tt.shidak_alpha(0.10), tt.summarized_pvals)\n",
    "    \n",
    "# Sort tests, so we have defined ordering\n",
    "tests_srt = [(k,tests[k]) for k in tests]\n",
    "tests_srt.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_detections(selection, tests_srt, do_p):\n",
    "    ctests = project_tests(tests_srt, selection)\n",
    "    total_det = sum(selection) * len(tests_srt)\n",
    "    tests_undefined = collections.defaultdict(lambda: 0)  # tname -> # of NONE in test\n",
    "    total_def_det = 0\n",
    "\n",
    "    # Fails removal & report\n",
    "    for tname, tvals in ctests:\n",
    "        for idx, tval in enumerate(tvals):\n",
    "            if tval is None:\n",
    "                eidx = get_ex_byidx(selection, idx)\n",
    "                tests_undefined[tname] += 1\n",
    "                #print('%s : %s' % (tname, exps[eidx].name))\n",
    "            else:\n",
    "                total_def_det += 1\n",
    "    return total_def_det     \n",
    "    \n",
    "def get_detections(ctests, alpha):\n",
    "    totals = len(ctests) * len(ctests[0][1])\n",
    "    test_fails = [sum(1 for y in x[1] if y is None) for x in ctests]\n",
    "    tests_detections = [(x[0], \n",
    "                         sum(1 for y in x[1] if y is not None and y <= alpha) / (len(x[1]) - test_fails[i])) \n",
    "                        for i, x in enumerate(ctests) if (len(x[1]) - test_fails[i]) > 0]\n",
    "    return tests_detections\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc_exps = [2,3,7,8]\n",
    "aes_tests = [4,5,6]\n",
    "top_rounds = get_top_rounds(compress_fnc(exps, lambda x: x.exp_info.id in fnc_exps))\n",
    "low_rounds = get_low_rounds(compress_fnc(exps, lambda x: x.exp_info.id in fnc_exps))\n",
    "med_rounds = get_med_rounds(compress_fnc(exps, lambda x: x.exp_info.id in fnc_exps))\n",
    "tf_10mib = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and x.exp_info.osize=='10MiB')\n",
    "tf_100mib = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and x.exp_info.osize=='100MiB')\n",
    "tf_10mib_rand = filter_experiments(exps, lambda x: x.exp_info.id in aes_tests and x.exp_info.osize=='10MiB')\n",
    "tf_100mib_rand = filter_experiments(exps, lambda x: x.exp_info.id in aes_tests and x.exp_info.osize=='100MiB')\n",
    "tf_100mib_rand_sac = filter_experiments(exps, lambda x: x.exp_info.id in aes_tests and x.exp_info.osize=='100MiB' and x.exp_info.meth == 'sac')\n",
    "tf_8gib = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and x.exp_info.osize=='8GiB')\n",
    "tf_8gib_ctr_hw = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and x.exp_info.osize=='8GiB' and x.exp_info.meth in ['ctr', 'hw'])\n",
    "tf_8gib_aes = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and x.exp_info.osize=='8GiB' and 'AES' in x.exp_info.fnc_name)\n",
    "tf_topr = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (top_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round))\n",
    "tf_topr8 = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (top_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and x.exp_info.osize=='8GiB')\n",
    "tf_topr8_ctr = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (top_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and x.exp_info.osize=='8GiB' and x.exp_info.meth == 'ctr')\n",
    "tf_topr8_aes = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (top_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and 'AES' in x.exp_info.fnc_name and x.exp_info.osize=='8GiB')\n",
    "tf_lowr8_aes = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (low_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and 'AES' in x.exp_info.fnc_name and x.exp_info.osize=='8GiB')\n",
    "tf_topr8_aes_ctr = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (top_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and 'AES' in x.exp_info.fnc_name and x.exp_info.meth == 'ctr' and x.exp_info.osize=='8GiB')\n",
    "tf_lowr8_aes_ctr = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (low_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and 'AES' in x.exp_info.fnc_name and x.exp_info.meth == 'ctr' and x.exp_info.osize=='8GiB')\n",
    "tf_topr8_aes_hw = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (top_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and 'AES' in x.exp_info.fnc_name and x.exp_info.meth == 'hw' and x.exp_info.osize=='8GiB')\n",
    "tf_lowr8_aes_hw = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (low_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and 'AES' in x.exp_info.fnc_name and x.exp_info.meth == 'hw' and x.exp_info.osize=='8GiB')\n",
    "tf_lowr = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (low_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round))\n",
    "tf_lowr8 = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (low_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and x.exp_info.osize=='8GiB')\n",
    "tf_lowr8_ctr = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (low_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and x.exp_info.osize=='8GiB' and x.exp_info.meth == 'ctr')\n",
    "tf_medr8 = filter_experiments(exps, lambda x: x.exp_info.id in fnc_exps and (med_rounds[x.exp_info.fnc_name] == x.exp_info.fnc_round) and x.exp_info.osize=='8GiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = tf_lowr\n",
    "\n",
    "tests_detections = get_detections(project_tests(tests_srt, tf_lowr8), 0.01)\n",
    "fig, ax = pyplot.subplots(figsize=(6*3, 2*3))\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "plt.scatter([x[0] for x in tests_detections], \n",
    "            [x[1] for x in tests_detections], \n",
    "            label='plot', s=10)\n",
    "plt.legend(loc='best')\n",
    "ax.set_ylabel(\"relative detections (H0 rejections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter top 3 strongest and top 3 lowest tests, 10, 100 and 8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.batteries[20010].tests[200545].variants[501513].sub_tests[1081150].stats\n",
    "#loader.batteries[20010].tests[200545].summarized_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step =0.0001\n",
    "alpha=0.0001 - step\n",
    "\n",
    "selection = tf_100mib  # tf_100mib_rand\n",
    "\n",
    "ctests = project_tests(tests_srt, selection)\n",
    "total_det = sum(selection) * len(tests_srt)\n",
    "tests_undefined = collections.defaultdict(lambda: 0)  # tname -> # of NONE in test\n",
    "total_def_det = 0\n",
    "\n",
    "# Fails removal & report\n",
    "for tname, tvals in ctests:\n",
    "    for idx, tval in enumerate(tvals):\n",
    "        if tval is None:\n",
    "            eidx = get_ex_byidx(selection, idx)\n",
    "            tests_undefined[tname] += 1\n",
    "            #print('%s : %s' % (tname, exps[eidx].name))\n",
    "        else:\n",
    "            total_def_det += 1\n",
    "            \n",
    "print('Total detections: %s, defined: %s' % (total_det, total_def_det))\n",
    "\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "while alpha <= 1.:\n",
    "    alpha += step\n",
    "    \n",
    "    tests_detections = [(x[0], sum(1 for y in x[1] if y is not None and y < alpha)) for x in ctests]\n",
    "    detections = sum([x[1] for x in tests_detections])\n",
    "    \n",
    "    if min([abs(alpha - x) for x in [0.005, 0.001, 0.01, 0.05, 0.1, 0.5, 1.]]) <= 0.000001:\n",
    "        print('Alpha: %0.8f, detections: %s, relative: %s' % (alpha, detections, float(detections)/total_def_det if total_def_det > 0 else 0))\n",
    "        \n",
    "    x_data.append(alpha)\n",
    "    #y_data.append(float(detections))\n",
    "    #y_data.append(alpha / (float(detections)/total_def_det))\n",
    "    #y_data.append((float(detections)/total_def_det) / alpha)\n",
    "    y_data.append((float(detections)/total_def_det) if total_def_det else 0)\n",
    "    \n",
    "fig, ax = pyplot.subplots(figsize=(6*2, 4*2))\n",
    "plt.scatter(x_data, \n",
    "            y_data, \n",
    "            label='plot', s=4)\n",
    "plt.plot(x_data, x_data, label='y=x', color='red')\n",
    "plt.legend(loc='best')  # plt.tick_params(labelsize=14)\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"relative detections (H0 rejections)\")\n",
    "# TODO: compute KS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: work in the test matrix in columns, count number of test reports per data set and assign points.\n",
    "# Tests ranking results together.\n",
    "# T1: per data: score tests. N points, divide between activated tests\n",
    "# T2: per data: add tests to the set. if set exists, increment number. Which are the most often sets?\n",
    "# T3: per data: triangular matrix of N tests. add +1 to a test which fired together. Which are the most firing tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = tf_lowr8  # tf_8gib\n",
    "alpha = 0.01 \n",
    "\n",
    "tests_scoring1 = collections.defaultdict(lambda: 0)\n",
    "tests_scoring2 = collections.defaultdict(lambda: 0)\n",
    "tests_scoring_eq = collections.defaultdict(lambda: 0)\n",
    "tests_sets = collections.defaultdict(lambda: 0)\n",
    "tests_matrix = [[0]*len(tests_srt) for i in range(len(tests_srt))]\n",
    "tests_matrix_2 = [[0]*len(tests_srt) for i in range(len(tests_srt))]\n",
    "\n",
    "ctests = project_tests(tests_srt, selection)\n",
    "total_det = sum(selection) * len(tests_srt)\n",
    "tests_undefined = collections.defaultdict(lambda: 0) \n",
    "num_tests = len(ctests)\n",
    "\n",
    "for col in range(len(ctests[0][1])):\n",
    "    num_defined = sum(1 for i in range(num_tests) if ctests[i][1][col] is not None)\n",
    "    detections = [i for i in range(num_tests) if ctests[i][1][col] is not None and ctests[i][1][col] < alpha]\n",
    "    num_detections = len(detections)\n",
    "    \n",
    "    # T1\n",
    "    points_to_add1 = num_defined // num_detections if num_detections > 0 else 0\n",
    "    points_to_add2 = num_defined // (num_defined-num_detections) if num_defined != num_detections > 0 else 0\n",
    "    for ti in range(num_tests):\n",
    "        tests_scoring_eq[ti] += points_to_add1 if ti in detections else points_to_add2\n",
    "        tests_scoring1[ti] += points_to_add1 if ti in detections else 0\n",
    "        tests_scoring2[ti] += 0 if ti in detections else points_to_add2\n",
    "        \n",
    "    # T2\n",
    "    tests_sets[tuple(detections)] += 1\n",
    "    \n",
    "    # T3\n",
    "    for ti in detections:\n",
    "        for ti2 in detections:\n",
    "            if ti > ti2:\n",
    "                continue\n",
    "            tests_matrix[ti][ti2] += 1\n",
    "    \n",
    "    pairs = set([(i, j) for i,j in itertools.product(detections, detections) if i<=j])\n",
    "    for i in range(len(ctests)):\n",
    "        for j in range(i, len(ctests)):\n",
    "            if (i,j) not in pairs:\n",
    "                tests_matrix_2[i][j] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring by rounds achieved\n",
    "def round_scoring(exps, tests_srt, selection, alpha=0.01, desc=None):\n",
    "    ctests = project_tests(tests_srt, selection)\n",
    "    total_det = sum(selection) * len(tests_srt)\n",
    "    tests_undefined = collections.defaultdict(lambda: 0) \n",
    "    num_tests = len(ctests)\n",
    "\n",
    "    fnc_rounds = get_rounds(exps)\n",
    "    f2r = collections.defaultdict(lambda: [])  # fname -> rounds[] -> exp IDs\n",
    "    for ix, x in enumerate(exps):\n",
    "        f2r[x.exp_info.fnc_name].append((x, ix))\n",
    "\n",
    "    f2r_map = {fname: sorted(f2r[fname], key=lambda x: (x[0].exp_info.fnc_round, x[0].exp_info.meth)) for fname in f2r}\n",
    "\n",
    "    # testidx -> fname -> top round achieved\n",
    "    test_rounds = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(\n",
    "            lambda: -1))\n",
    "    \n",
    "    test_occurences = collections.defaultdict(lambda: 0)\n",
    "    test_undefined = collections.defaultdict(lambda: 0)\n",
    "    test_undefineds = collections.defaultdict(lambda: [])\n",
    "    test_defined = collections.defaultdict(lambda: 0)\n",
    "    test_detected = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    for fname in f2r_map:\n",
    "        for exp, exid in f2r_map[fname]:\n",
    "            nexid = get_ex_newidx(selection, exid)  # after compression by selection\n",
    "            if nexid is None:\n",
    "                continue\n",
    "                \n",
    "            fround = exp.exp_info.fnc_round\n",
    "            for testid in range(len(ctests)):\n",
    "                tname = ctests[testid][0]\n",
    "                pvals = ctests[testid][1]\n",
    "                \n",
    "                test_occurences[testid] += 1\n",
    "                if pvals[nexid] is None:\n",
    "                    test_undefined[testid] += 1\n",
    "                    test_undefineds[testid].append(exp)\n",
    "                else:\n",
    "                    test_defined[testid] += 1\n",
    "                    \n",
    "                if pvals[nexid] is not None and pvals[nexid] < alpha:\n",
    "                    test_detected[testid] += 1\n",
    "                    test_rounds[testid][fname] = max(fround, test_rounds[testid][fname])\n",
    "    \n",
    "    max_points = 0\n",
    "    for fname in f2r_map:\n",
    "        fr = fnc_rounds[fname]\n",
    "        maxr = max(fr)\n",
    "        minr = min(fr)\n",
    "        max_points += 1 + (maxr - minr)\n",
    "    print('Max points: %s' % max_points)\n",
    "    \n",
    "    tis = collections.defaultdict(lambda: [])\n",
    "    for fname in f2r_map:\n",
    "        fr = fnc_rounds[fname]\n",
    "        maxr = max(fr)\n",
    "        minr = min(fr)   \n",
    "        maxdet = max([test_rounds[testid][fname] for testid in range(len(ctests))])\n",
    "        tis['%s|r%s' % (fname, maxdet)] = [t[0] for ix, t in enumerate(ctests) if test_rounds[ix][fname] == maxdet]\n",
    "    #print(json.dumps(tis, indent=2))\n",
    "    \n",
    "    #print(json.dumps(test_rounds, indent=2))\n",
    "    tests_scoring_r = collections.defaultdict(lambda: 0)\n",
    "    for fname in f2r_map:\n",
    "        fr = fnc_rounds[fname]\n",
    "        maxr = max(fr)\n",
    "        minr = min(fr)        \n",
    "        for testid in range(len(ctests)):\n",
    "            if test_rounds[testid][fname] < 0 or test_rounds[testid][fname] is None:\n",
    "                continue\n",
    "            points = 1 * (1 + (test_rounds[testid][fname] - minr))\n",
    "            tests_scoring_r[tests_srt[testid][0]] += points\n",
    "            \n",
    "    #print(json.dumps(tests_scoring_r, indent=2))\n",
    "    \n",
    "    fig, ax = pyplot.subplots(figsize=(6*3, 2*3))\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "    \n",
    "    acc_data = [(x[0], tests_scoring_r[x[0]]) for x in ctests]\n",
    "    acc_data.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    #sns.barplot(x=[x[0] for x in ctests], y=[tests_scoring_r[x[0]] for x in ctests])\n",
    "    sns.barplot(x=[x[0] for x in acc_data], y=[x[1] for x in acc_data])\n",
    "    ax.set_ylabel(\"test scoring (on detected function rounds) %s\" % (desc if desc else ''))\n",
    "    \n",
    "    print(acc_data)\n",
    "    for ix, x in enumerate(ctests):\n",
    "        print('Records: %s  Undefined: %s Defined: %s Detections: %s %s  %s' \n",
    "              % (test_occurences[ix], test_undefined[ix], test_defined[ix], test_detected[ix], x[0],\n",
    "                 json.dumps([str(x) for x in test_undefineds[ix]], indent=2)))\n",
    "    #plt.savefig(\"/tmp/sorted.png\", bbox_inches='tight', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_scoring(exps, tests_srt, tf_8gib_ctr_hw, alpha=0.001, desc='8gib_ctr_hw')  # tf_8gib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tf_8gib_ctr_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1 test uniqueness scoring\n",
    "fig, ax = pyplot.subplots(figsize=(6*3, 2*3))\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "sns.barplot(x=[x[0] for x in ctests], y=[tests_scoring1[x] for x in range(len(ctests))])\n",
    "ax.set_ylabel(\"test scoring (the better the more unique appearances)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2: test sets\n",
    "tests_sets_l = sorted([(k, tests_sets[k]) for k in tests_sets if tests_sets[k]>=2], reverse=True)\n",
    "print(tests_sets_l[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T3: matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G2 = Graph('G')\n",
    "for t in ctests:\n",
    "    G.add_node(t[0])\n",
    "\n",
    "for i in range(len(ctests)):\n",
    "    for j in range(i+1, len(ctests)):\n",
    "        w = tests_matrix[i][j]\n",
    "        if w < 75:\n",
    "            continue\n",
    "            \n",
    "        print(\"%s - %s [%s]\" % (ctests[i][0], ctests[j][0], w))\n",
    "        G.add_edge(ctests[i][0], ctests[j][0], weight=w, color='green' if w<=5 else 'black', size=10 if w<=5 else 2)\n",
    "        G2.edge(str(ctests[i][0]), str(ctests[j][0]), penwidth=('%s' % (w-74))) # weight=w)\n",
    "\n",
    "#print(json.dumps([list(x) for x in nx.connected_components(G)], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2.engine = 'circo'\n",
    "#G2.engine = 'neato'\n",
    "#G2.engine = 'fdp'\n",
    "#G2.engine = 'osage'\n",
    "###G2.engine = 'patchwork'\n",
    "#G2.engine = 'sfdp'\n",
    "#G2.engine = 'twopi'\n",
    "G2.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 24))\n",
    "#nx.draw_networkx(G, with_labels=True)\n",
    "#nx.draw(G, with_labels=True)\n",
    "nx.draw(G, with_labels=True, pos=nx.circular_layout(G))\n",
    "#nx.draw(G, with_labels=True, pos=nx.kamada_kawai_layout(G))\n",
    "#nx.draw(G, with_labels=True, pos=nx.shell_layout(G, shells))\n",
    "#nx.draw(self.G, with_labels=True, pos=mylay(self.shells))\n",
    "#plt.savefig(\"/tmp/deps.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIDAK TEST\n",
    "alpha= 0.0085 #.015  # .005\n",
    "pvals=[.03, .5, .21, ] + [0.001, 0.11, 0.99,]\n",
    "nvals=[.07, .02, .002009, 0.98, 0.55,   0.0001 ]\n",
    "ns_ex=sidak_alpha(alpha, len(nvals))\n",
    "ns_cp=sidak_alpha(min(nvals), len(nvals))\n",
    "nsicp=sidak_inv(min(nvals), len(nvals))\n",
    "         \n",
    "print('alpha: %s' % alpha)\n",
    "print('min p: %s' % min(nvals))\n",
    "print('sh  t: %s, %s' % (ns_ex, min(nvals) < ns_ex))\n",
    "print('shi p: %s, %s' % (nsicp, nsicp < alpha))\n",
    "print('')\n",
    "\n",
    "t1 = pvals+nvals\n",
    "a1 = sidak_alpha(alpha, len(t1))\n",
    "print(min(t1), a1, min(t1) < a1, len(t1), \"\\n\")\n",
    "\n",
    "t2 = pvals+[sidak_inv(min(nvals), len(nvals))]\n",
    "a2 = sidak_alpha(alpha, len(t2))\n",
    "print(min(t2), a2, min(t2) < a2, len(t2), \"\\n\")\n",
    "\n",
    "dexp = (len(t1)) / (len(pvals)+1.)\n",
    "Y = 1-(1-min(t1))**(dexp)\n",
    "t3 = pvals+[Y]\n",
    "a3 = sidak_alpha(alpha, len(t3))\n",
    "print(Y)\n",
    "print(min(t3), a2, min(t3) < a3, len(t3), \"\\n\")\n",
    "#print(sidak_inv(0.1, 5))\n",
    "#print(sidak_inv(0.01, 5))\n",
    "\n",
    "#random.shuffle(t1)\n",
    "print('='*80)\n",
    "print('LEN: %s' % len(t1))\n",
    "ideal = merge_pvals(t1, 999)\n",
    "print('M: ', ideal) # FULL merge\n",
    "#ammend(t1)\n",
    "#print('M: ', merge_pvals(t1, 999)) # FULL merge\n",
    "print('M: ', merge_pvals(t1, 2))\n",
    "print('M: ', merge_pvals(t1, 3))\n",
    "print('M: ', merge_pvals(t1, 4))\n",
    "print('M: ', merge_pvals(t1, 5))\n",
    "print('M: ', merge_pvals(t1, 6))\n",
    "print('M: ', merge_pvals(t1, 7))\n",
    "print('M: ', merge_pvals(t1, 8))\n",
    "print('M: ', merge_pvals(t1, 9))\n",
    "print('M: ', merge_pvals(t1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_chars = {\n",
    "    \"False|Anderson-Darling (A2)|Kolmogorov-Smirnov (D+)|Kolmogorov-Smirnov (D-)|Std empirical corr|Std empirical mean\": 12372,\n",
    "    \"False|Chi-square\": 49314,\n",
    "    \"False|Normal\": 19811,\n",
    "    \"False|Chi-square(C)|Chi-square(H)|Chi-square(J)|Chi-square(M)|Chi-square(R)\": 25980,\n",
    "    \"True|Chi-Square\": 247182,\n",
    "    \"False|Collision\": 10350,\n",
    "    \"False|Bit distance\": 3489,\n",
    "    \"False|Chi-square|Normal\": 3042,\n",
    "    \"False|Anderson-Darling|Kolmogorov-Smirnov (D+)|Kolmogorov-Smirnov (D-)\": 2613,\n",
    "    \"False|Chi-square|Global longest run of 1\": 2184,\n",
    "    \"False|Anderson-Darling|Kolmogorov-Smirnov (D+)|Kolmogorov-Smirnov (D-)|Normal|Sample variance\": 3029,\n",
    "    \"False|Anderson-Darling (A2) (bits)|Anderson-Darling (A2) (runs)|Chi-square (runs)|Kolmogorov-Smirnov (D+) (bits)|Kolmogorov-Smirnov (D+) (runs)|Kolmogorov-Smirnov (D-) (bits)|Kolmogorov-Smirnov (D-) (runs)|Normal (bits)|Sample variance (bits)\": 442,\n",
    "    \"False|Anderson-Darling|Chi-square\": 1724,\n",
    "    \"True|Kolmogorov-Smirnov\": 121542,\n",
    "    \"False|A2 (m-NP)|A2 (mNP1)|AD (NP)|AD (mNP2)|Jumps Y (mN)\": 413,\n",
    "    \"False|A2 (m-NP)|A2 (mNP1)|AD (NP)|AD (mNP2)|AD (mNP2-S)|Jumps Y (mN)\": 824,\n",
    "    \"False|Anderson-Darling - AD (A2)|Anderson-Darling - KS (D+)|Anderson-Darling - KS (D-)|Chi-square - AD (A2)|Chi-square - KS (D+)|Chi-square - KS (D-)|Observations sums\": 858,\n",
    "    \"False|Anderson-Darling\": 858,\n",
    "    \"False|Anderson-Darling|Chi-square|Kolmogorov-Smirnov (D+)|Kolmogorov-Smirnov (D-)\": 858,\n",
    "    \"False|Unknown 0|Unknown 1|Unknown 2|Unknown 3\": 1,\n",
    "    \"False|Unknown 0\": 26520,\n",
    "    \"False|Unknown 0|Unknown 1\": 884\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi=0\n",
    "kolm=0\n",
    "ander=0\n",
    "other=0\n",
    "for k in res_chars:\n",
    "    kl = k.lower()\n",
    "    if 'chi' in kl:\n",
    "        chi+=res_chars[k]\n",
    "    elif 'kolm' in kl:\n",
    "        kolm+=res_chars[k]\n",
    "    elif 'ander' in kl:\n",
    "        ander+=res_chars[k]\n",
    "    else:\n",
    "        other+=res_chars[k]\n",
    "        \n",
    "print(chi, kolm, ander,other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_sum = collections.defaultdict(lambda: 0)\n",
    "for tt in loader.tests.values():\n",
    "    \n",
    "    tt_id = '|'.join(reversed(tt.short_desc()))\n",
    "    for vv in tt.variants.values():\n",
    "        cfv = '|'.join([str(x) for x in vv.settings.values_tuple()])\n",
    "\n",
    "        for ss in vv.sub_tests.values():\n",
    "            tfv = '|'.join([str(x) for x in ss.params.values_tuple()])\n",
    "            if len(ss.stats) == 0:\n",
    "                logger.debug('Null statistics for test %s:%s:%s' % (tt_id, cfv, tfv))\n",
    "                continue\n",
    "\n",
    "            picked = dump_data.pick_one_statistic(ss.stats)\n",
    "            picked_sum[picked.name] += 1\n",
    "\n",
    "print(json.dumps(picked_sum, indent=2))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_extended(tests, add_all=False, pick_one=False, add_passed=True):\n",
    "    for tt in tests.values():\n",
    "        tt_id = '|'.join(reversed(tt.short_desc()))\n",
    "        tt.summarized_pvals = []\n",
    "        tt.summarized_passed= []\n",
    "\n",
    "        for vv in tt.variants.values():\n",
    "            # cfs = '|'.join([str(x) for x in vv.settings.keys_tuple()])\n",
    "            cfv = '|'.join([str(x) for x in vv.settings.values_tuple()])\n",
    "\n",
    "            for ss in vv.sub_tests.values():\n",
    "                # tfs = '|'.join([str(x) for x in ss.params.keys_tuple()])\n",
    "                tfv = '|'.join([str(x) for x in ss.params.values_tuple()])\n",
    "                if len(ss.stats) == 0:\n",
    "                    logger.debug('Null statistics for test %s:%s:%s' % (tt_id, cfv, tfv))\n",
    "                    continue\n",
    "\n",
    "                if not add_all and pick_one:\n",
    "                    picked = pick_one_statistic(ss.stats)\n",
    "                    #self.picked_stats[picked.name] += 1\n",
    "                \n",
    "                desc = '{vid:%s|cfg:%s}{subid:%s|cfg:%s}' % (vv.id, cfv, ss.id, tfv)\n",
    "                picked_stats = loader.pick_stats(ss.stats, add_all=add_all, pick_one=pick_one)\n",
    "                picked_pvals = [x.value for x in picked_stats]\n",
    "                picked_pass = [(x.passed, desc) for x in picked_stats]\n",
    "\n",
    "                # Sidak postprocessing.\n",
    "                # Compute resulting p-value from all pvalues in collected stats.\n",
    "                # If pvalues are independent, result are better and we can compute one final pvalue.\n",
    "                # WARNING: this strategy does not work well if resulting tree is unbalanced.\n",
    "                #          It has to be perfectly symmetric.\n",
    "                if not add_all and not pick_one:\n",
    "                    picked_pvals = [sidak_inv(min(picked_pvals), len(picked_pvals))]\n",
    "                    picked_pass = []\n",
    "\n",
    "                tt.summarized_pvals += picked_pvals\n",
    "                if add_passed:\n",
    "                    tt.summarized_passed += picked_pass\n",
    "                    \n",
    "process_extended(loader.tests, add_all=True, add_passed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syso format dump\n",
    "# {\n",
    "#     \"test\": \"TestU01 Alphabit|smultin_MultinomialBitsOver\",\n",
    "#     \"subtest\": \"3|cfg:16|1|FALSE|10000000|0|32\",\n",
    "#     \"variant\": \"cfg:10000000|0|32|1\",\n",
    "#     \"data_type\": \"10485760|ctr|Grostl\",\n",
    "# }\n",
    "\n",
    "def process_syso(tests):\n",
    "    res = []\n",
    "    resmap = {}\n",
    "    \n",
    "    for ttidx, tt in enumerate(tests.values()):\n",
    "        tt_id = '|'.join(reversed(tt.short_desc()))\n",
    "\n",
    "        for vvidx, vv in enumerate(tt.variants.values()):  # variant\n",
    "            cfs = '|'.join([str(x) for x in vv.settings.keys_tuple()])\n",
    "            cfv = '|'.join([str(x) for x in vv.settings.values_tuple()])\n",
    "    \n",
    "            tmpsubs = []\n",
    "            for ssidx, ss in enumerate(vv.sub_tests.values()):  # subtest\n",
    "                tfv = '|'.join([str(x) for x in ss.params.values_tuple()])\n",
    "                tmpsubs.append((tfv, ss))\n",
    "            \n",
    "            subs_acc = []\n",
    "            sorter = lambda x: x[0]\n",
    "            for k, g in itertools.groupby(sorted(tmpsubs, key=sorter), sorter):\n",
    "                subs_acc.append([x[1] for x in g])\n",
    "            \n",
    "            for skidx, gs in enumerate(subs_acc):\n",
    "                ss = gs[0]\n",
    "                tfs = '|'.join([str(x) for x in ss.params.keys_tuple()])\n",
    "                tfv = '|'.join([str(x) for x in ss.params.values_tuple()])\n",
    "                \n",
    "                tsubs = []\n",
    "                cobj = collections.OrderedDict([\n",
    "                        ('test', tt_id),\n",
    "                        ('subtest', '%s' % tfv),\n",
    "                        ('subtest_type', tfs),\n",
    "                        ('variant', cfv),\n",
    "                        ('variant_type', cfs),\n",
    "                        ('variant_id', vv.id),\n",
    "                        ('exid', tt.battery.exp.id),\n",
    "                        ('data_type', tt.battery.exp.name),\n",
    "                        ('subs', tsubs),\n",
    "                    ])\n",
    "                \n",
    "                for ssidx, ss in enumerate(gs):\n",
    "                    cstats = [collections.OrderedDict([\n",
    "                       ('name', _st.name),\n",
    "                       ('value', _st.value),\n",
    "                       ('pass', _st.passed),\n",
    "                    ]) for _st in ss.stats]\n",
    "                    \n",
    "                    csub = collections.OrderedDict([\n",
    "                        ('sid', ss.id),\n",
    "                        ('idx', ss.idx),\n",
    "                        ('pvals', ss.pvals),\n",
    "                        ('stats', cstats)\n",
    "                    ]) \n",
    "                    tsubs.append(csub)\n",
    "                    \n",
    "                res.append(cobj)\n",
    "                \n",
    "        #if ttidx > 1:\n",
    "        #    break\n",
    "    return res\n",
    "\n",
    "res = process_syso(loader.tests)\n",
    "json.dump(res, open('syso-full-10-11-12.json', 'w+'), indent=2)\n",
    "\n",
    "#print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syso json\n",
    "# tests[test-name][experiment-name:size-meth-fnc-round]\n",
    "# tests = [test-name][exp-idx] -> pvalue\n",
    "\n",
    "tests_mult = collections.defaultdict(lambda: [None] * len(exps))\n",
    "for tt in loader.tests.values():\n",
    "    tt_id = '|'.join(reversed(tt.short_desc()))\n",
    "    tests_mult[tt_id][rev_exp[tt.battery.exp.name]] = list(zip(tt.summarized_pvals, tt.summarized_passed))\n",
    "\n",
    "sys_grouper = lambda x: (x.size, x.meth, x.fnc_name, x.fnc_round)\n",
    "exps_sys = sorted(exps, key=lambda x: sys_grouper(x.exp_info))\n",
    "\n",
    "all_pvals = []\n",
    "tests_to_use = tests_mult\n",
    "tests_sys = collections.defaultdict(lambda: collections.defaultdict(lambda: list))\n",
    "for ti, tt in enumerate(tests_to_use):\n",
    "    \n",
    "    # Sort so the experiments that should be grouped are next to each other. Only for valid pvalues (test finished)\n",
    "    cur = sorted([(eidx, pval) for eidx, pval in enumerate(tests_to_use[tt]) if pval is not None], key=lambda x: sys_grouper(exps[x[0]].exp_info))\n",
    "    \n",
    "    # Group by the experiments with same size-meth-fnc-round\n",
    "    for k, g in itertools.groupby(cur, key=lambda x: sys_grouper(exps[x[0]].exp_info)):\n",
    "        g = list(g)\n",
    "        ckey = '|'.join([str(x) for x in k])\n",
    "        tests_sys[tt][ckey] = [x[1] for x in g]\n",
    "        for l1 in [x[1] for x in g]:\n",
    "            for l2 in l1:\n",
    "                all_pvals.append(l2[0])\n",
    "\n",
    "json.dump(tests_sys, open('syso-secmargin-pvals-ex-10-11-12.json', 'w+'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(x):\n",
    "    return sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([x for x in all_pvals if x > 0.9999], bins=25, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('Test config vars:')\n",
    "print(json.dumps(test_configs_var, indent=2))\n",
    "\n",
    "print('='*80)\n",
    "print('Test config types:')\n",
    "lst = sorted(set(list(flat_configs_types['ALL'].keys())))\n",
    "print(json.dumps(lst, indent=2))\n",
    "\n",
    "print('='*80)\n",
    "print('Test configs:')\n",
    "lst = sorted(set(list(flat_configs['ALL'].keys())))\n",
    "print(json.dumps(lst, indent=2))\n",
    "\n",
    "print('='*80)\n",
    "print('Flat configs:')\n",
    "print(json.dumps(flat_configs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(tests_mult, open('sysotmp2.json', 'w+'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp='''697852938\t0.004914669785648584\t3025838\n",
    "697852939\t0.006502269767224789\t3025838\n",
    "697852940\t0.014632189646363258\t3025838\n",
    "697852941\t0.016551120206713676\t3025838\n",
    "697852942\t0.0186814796179533\t3025838\n",
    "697852943\t0.019112680107355118\t3025838\n",
    "697852944\t0.02417387068271637\t3025838\n",
    "697852945\t0.02553240954875946\t3025838\n",
    "697852946\t0.029523009434342384\t3025838\n",
    "697852947\t0.03490941971540451\t3025838\n",
    "697852948\t0.04425868019461632\t3025838\n",
    "697852949\t0.04603486880660057\t3025838\n",
    "697852950\t0.047173768281936646\t3025838\n",
    "697852951\t0.04951927065849304\t3025838\n",
    "697852952\t0.05146192014217377\t3025838\n",
    "697852953\t0.053719330579042435\t3025838\n",
    "697852954\t0.05765518918633461\t3025838\n",
    "697852955\t0.05792544037103653\t3025838\n",
    "697852956\t0.07670781761407852\t3025838\n",
    "697852957\t0.0794355496764183\t3025838\n",
    "697852958\t0.08880814909934998\t3025838\n",
    "697852959\t0.08994050323963165\t3025838\n",
    "697852960\t0.09938798099756241\t3025838\n",
    "697852961\t0.1060771495103836\t3025838\n",
    "697852962\t0.10736676305532455\t3025838\n",
    "697852963\t0.1095411628484726\t3025838\n",
    "697852964\t0.11443573236465454\t3025838\n",
    "697852965\t0.11763039976358414\t3025838\n",
    "697852966\t0.11948402225971222\t3025838\n",
    "697852967\t0.13054220378398895\t3025838\n",
    "697852968\t0.15809056162834167\t3025838\n",
    "697852969\t0.1620711386203766\t3025838\n",
    "697852970\t0.1632203459739685\t3025838\n",
    "697852971\t0.16437481343746185\t3025838\n",
    "697852972\t0.16437481343746185\t3025838\n",
    "697852973\t0.1649540364742279\t3025838\n",
    "697852974\t0.1684570610523224\t3025838\n",
    "697852975\t0.19046929478645325\t3025838\n",
    "697852976\t0.22242805361747742\t3025838\n",
    "697852977\t0.22452212870121002\t3025838\n",
    "697852978\t0.2308710366487503\t3025838\n",
    "697852979\t0.23444108664989471\t3025838\n",
    "697852980\t0.23515872657299042\t3025838\n",
    "697852981\t0.2380414456129074\t3025838\n",
    "697852982\t0.23876513540744781\t3025838\n",
    "697852983\t0.23949004709720612\t3025838\n",
    "697852984\t0.24167191982269287\t3025838\n",
    "697852985\t0.24902215600013733\t3025838\n",
    "697852986\t0.24976366758346558\t3025838\n",
    "697852987\t0.2534887492656708\t3025838\n",
    "697852988\t0.2633088529109955\t3025838\n",
    "697852989\t0.26867592334747314\t3025838\n",
    "697852990\t0.2955014705657959\t3025838\n",
    "697852991\t0.3003603219985962\t3025838\n",
    "697852992\t0.30361947417259216\t3025838\n",
    "697852993\t0.314318984746933\t3025838\n",
    "697852994\t0.3193110525608063\t3025838\n",
    "697852995\t0.32433614134788513\t3025838\n",
    "697852996\t0.3353331685066223\t3025838\n",
    "697852997\t0.34217217564582825\t3025838\n",
    "697852998\t0.34303078055381775\t3025838\n",
    "697852999\t0.34647315740585327\t3025838\n",
    "697853000\t0.3779962360858917\t3025838\n",
    "697853001\t0.3806636333465576\t3025838\n",
    "697853002\t0.38333672285079956\t3025838\n",
    "697853003\t0.4076303541660309\t3025838\n",
    "697853004\t0.4139912724494934\t3025838\n",
    "697853005\t0.41672444343566895\t3025838\n",
    "697853006\t0.42128854990005493\t3025838\n",
    "697853007\t0.42403215169906616\t3025838\n",
    "697853008\t0.4304480254650116\t3025838\n",
    "697853009\t0.43136611580848694\t3025838\n",
    "697853010\t0.4387238025665283\t3025838\n",
    "697853011\t0.44241076707839966\t3025838\n",
    "697853012\t0.45627880096435547\t3025838\n",
    "697853013\t0.4581322968006134\t3025838\n",
    "697853014\t0.4655549228191376\t3025838\n",
    "697853015\t0.48974794149398804\t3025838\n",
    "697853016\t0.4972037076950073\t3025838\n",
    "697853017\t0.5823636054992676\t3025838\n",
    "697853018\t0.5887377262115479\t3025838\n",
    "697853019\t0.6014142036437988\t3025838\n",
    "697853020\t0.6483394503593445\t3025838\n",
    "697853021\t0.658685564994812\t3025838\n",
    "697853022\t0.6806889176368713\t3025838\n",
    "697853023\t0.7109201550483704\t3025838\n",
    "697853024\t0.7366911768913269\t3025838\n",
    "697853025\t0.7691289782524109\t3025838\n",
    "697853026\t0.7803466320037842\t3025838\n",
    "697853027\t0.7972322106361389\t3025838\n",
    "697853028\t0.8219696283340454\t3025838\n",
    "697853029\t0.8237902522087097\t3025838\n",
    "697853030\t0.8491302728652954\t3025838\n",
    "697853031\t0.8598040342330933\t3025838\n",
    "697853032\t0.8728957772254944\t3025838\n",
    "697853033\t0.8837463855743408\t3025838\n",
    "697853034\t0.9249604940414429\t3025838\n",
    "697853035\t0.9500006437301636\t3025838\n",
    "697853036\t0.9639958143234253\t3025838\n",
    "697853037\t0.9678769111633301\t3025838'''\n",
    "\n",
    "\n",
    "inp='''697853338\t0.000011250000170548446\t3025842\n",
    "697853339\t0.00350211001932621\t3025842\n",
    "697853340\t0.009129649959504604\t3025842\n",
    "697853341\t0.010111579671502113\t3025842\n",
    "697853342\t0.010273230262100697\t3025842\n",
    "697853343\t0.025871990248560905\t3025842\n",
    "697853344\t0.028826050460338593\t3025842\n",
    "697853345\t0.030500030145049095\t3025842\n",
    "697853346\t0.04602126032114029\t3025842\n",
    "697853347\t0.04904283955693245\t3025842\n",
    "697853348\t0.05152222886681557\t3025842\n",
    "697853349\t0.056505098938941956\t3025842\n",
    "697853350\t0.0834764614701271\t3025842\n",
    "697853351\t0.085155189037323\t3025842\n",
    "697853352\t0.08632293343544006\t3025842\n",
    "697853353\t0.08964061737060547\t3025842\n",
    "697853354\t0.09910307079553604\t3025842\n",
    "697853355\t0.10003849864006042\t3025842\n",
    "697853356\t0.10338187962770462\t3025842\n",
    "697853357\t0.11141431331634521\t3025842\n",
    "697853358\t0.11185982823371887\t3025842\n",
    "697853359\t0.11848258227109909\t3025842\n",
    "697853360\t0.12144789844751358\t3025842\n",
    "697853361\t0.17934998869895935\t3025842\n",
    "697853362\t0.18065841495990753\t3025842\n",
    "697853363\t0.19085368514060974\t3025842\n",
    "697853364\t0.19850070774555206\t3025842\n",
    "697853365\t0.23529300093650818\t3025842\n",
    "697853366\t0.24311693012714386\t3025842\n",
    "697853367\t0.24556083977222443\t3025842\n",
    "697853368\t0.25612300634384155\t3025842\n",
    "697853369\t0.2598739266395569\t3025842\n",
    "697853370\t0.26305291056632996\t3025842\n",
    "697853371\t0.27871841192245483\t3025842\n",
    "697853372\t0.2806601822376251\t3025842\n",
    "697853373\t0.31267422437667847\t3025842\n",
    "697853374\t0.3130578398704529\t3025842\n",
    "697853375\t0.3209002912044525\t3025842\n",
    "697853376\t0.3295722007751465\t3025842\n",
    "697853377\t0.34895023703575134\t3025842\n",
    "697853378\t0.34903621673583984\t3025842\n",
    "697853379\t0.36707979440689087\t3025842\n",
    "697853380\t0.3728322386741638\t3025842\n",
    "697853381\t0.3912302255630493\t3025842\n",
    "697853382\t0.3987952470779419\t3025842\n",
    "697853383\t0.41622036695480347\t3025842\n",
    "697853384\t0.41763949394226074\t3025842\n",
    "697853385\t0.431035578250885\t3025842\n",
    "697853386\t0.4445094168186188\t3025842\n",
    "697853387\t0.4635542035102844\t3025842\n",
    "697853388\t0.4677604138851166\t3025842\n",
    "697853389\t0.47136151790618896\t3025842\n",
    "697853390\t0.47390589118003845\t3025842\n",
    "697853391\t0.4776434898376465\t3025842\n",
    "697853392\t0.47958147525787354\t3025842\n",
    "697853393\t0.4808431565761566\t3025842\n",
    "697853394\t0.48542582988739014\t3025842\n",
    "697853395\t0.5012537837028503\t3025842\n",
    "697853396\t0.5057639479637146\t3025842\n",
    "697853397\t0.5154284834861755\t3025842\n",
    "697853398\t0.5185425281524658\t3025842\n",
    "697853399\t0.5372365713119507\t3025842\n",
    "697853400\t0.5520502328872681\t3025842\n",
    "697853401\t0.5642455220222473\t3025842\n",
    "697853402\t0.5806460380554199\t3025842\n",
    "697853403\t0.5911000967025757\t3025842\n",
    "697853404\t0.6054104566574097\t3025842\n",
    "697853405\t0.6066920757293701\t3025842\n",
    "697853406\t0.6142104268074036\t3025842\n",
    "697853407\t0.6163209676742554\t3025842\n",
    "697853408\t0.6364975571632385\t3025842\n",
    "697853409\t0.6469718813896179\t3025842\n",
    "697853410\t0.6490170955657959\t3025842\n",
    "697853411\t0.6604179739952087\t3025842\n",
    "697853412\t0.6644731163978577\t3025842\n",
    "697853413\t0.7009350061416626\t3025842\n",
    "697853414\t0.7076488733291626\t3025842\n",
    "697853415\t0.7148332595825195\t3025842\n",
    "697853416\t0.7203283309936523\t3025842\n",
    "697853417\t0.7401187419891357\t3025842\n",
    "697853418\t0.752748966217041\t3025842\n",
    "697853419\t0.7548877596855164\t3025842\n",
    "697853420\t0.760158360004425\t3025842\n",
    "697853421\t0.7782148718833923\t3025842\n",
    "697853422\t0.7878789901733398\t3025842\n",
    "697853423\t0.802317202091217\t3025842\n",
    "697853424\t0.8033522367477417\t3025842\n",
    "697853425\t0.8043506741523743\t3025842\n",
    "697853426\t0.8281975984573364\t3025842\n",
    "697853427\t0.8508448004722595\t3025842\n",
    "697853428\t0.8533149361610413\t3025842\n",
    "697853429\t0.8728923201560974\t3025842\n",
    "697853430\t0.8927019834518433\t3025842\n",
    "697853431\t0.9007689356803894\t3025842\n",
    "697853432\t0.9044573903083801\t3025842\n",
    "697853433\t0.913966178894043\t3025842\n",
    "697853434\t0.9309571981430054\t3025842\n",
    "697853435\t0.9310958981513977\t3025842\n",
    "697853436\t0.9456146359443665\t3025842\n",
    "697853437\t0.989915668964386\t3025842'''\n",
    "\n",
    "from scipy import stats\n",
    "_pvals=([float(x.split('\\t')[1]) for x in inp.split('\\n')])\n",
    "print(len(_pvals))\n",
    "\n",
    "_pvals2 = [random.random() for _ in range(100)]\n",
    "print(stats.kstest(_pvals, 'uniform', mode='asymp'))\n",
    "print(stats.kstest([0.5, 0.5], 'uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pvals=[ \n",
    "        0.10647586733102798,\n",
    "        0.16674716770648956,\n",
    "        0.19867828488349915,\n",
    "        0.20402072370052338,\n",
    "        0.2072029411792755,\n",
    "        0.30274155735969543,\n",
    "        0.3070487082004547,\n",
    "        0.3498513996601105,\n",
    "        0.3598391115665436,\n",
    "        0.3720359206199646,\n",
    "        0.4307703375816345,\n",
    "        0.5102576017379761,\n",
    "        0.5586630702018738,\n",
    "        0.5801839828491211,\n",
    "        0.584112823009491,\n",
    "        0.691771924495697,\n",
    "        0.6956953406333923,\n",
    "        0.7069452404975891,\n",
    "        0.7123063206672668,\n",
    "        0.7145896553993225,\n",
    "        0.7438201904296875,\n",
    "        0.9486949443817139,\n",
    "        0.9836832284927368,\n",
    "        0.9858270287513733\n",
    "      ]\n",
    "print(stats.kstest(_pvals, 'uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = json.load(open('passthenfail_pvals_subid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klinec(pvals):\n",
    "    nls = len(pvals)\n",
    "    spvals = sorted(pvals)\n",
    "    exdif = 1./(nls-1)\n",
    "    difs = []\n",
    "    \n",
    "    for i in range(1, len(pvals)):\n",
    "        cdif = spvals[i] - spvals[i-1]\n",
    "        difs.append(cdif)# - exdif)\n",
    "    \n",
    "    _av = np.average(difs)\n",
    "    std = np.std(difs)\n",
    "    print('Avg %s, std %s, ex: %s, len: %s' % (_av, std, exdif, nls))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.hist(difs, log=False)\n",
    "    #ax.hist(pvals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi\n",
    "\n",
    "_pvals2 = [random.random() for _ in range(1000)] + [0.9]*2\n",
    "#_pvals2 = np.random.normal(0, 1, 1000)\n",
    "#_pvals2 = chi.rvs(12, size=1000)\n",
    "print(stats.kstest(_pvals2, 'uniform'))\n",
    "print(klinec(_pvals2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pick_km_statistic2(stats):\n",
    "    if len(stats) == 0:\n",
    "        return None\n",
    "    for st in ['kolm']:\n",
    "        for cur in stats:\n",
    "            name = cur.name.lower()\n",
    "            if name.startswith(st):\n",
    "                return cur\n",
    "    return stats[0]\n",
    "\n",
    "mt6 = []\n",
    "for ix, ex in enumerate(js):\n",
    "    ex['pvals'] = []\n",
    "    ex['stats'] = []\n",
    "    ex['allstats'] = []\n",
    "    ex['stats_names'] = []\n",
    "    ex['my_ks'] = []\n",
    "    \n",
    "    for sid in ex['subid']:\n",
    "        sid = int(sid)\n",
    "        stest = loader.sids[sid]\n",
    "        st = pick_km_statistic2(stest.stats)\n",
    "        ex['pvals'].append(stest.pvals)\n",
    "        ex['stats'].append(st.value if st else None)\n",
    "        ex['stats_names'].append(st.name if st else None)\n",
    "        ex['allstats'].append([(x.name, x.value, x.passed) for x in stest.stats])\n",
    "        ex['my_ks'].append(stats.kstest(stest.pvals, 'uniform') if stest.pvals else None)\n",
    "        if len(stest.pvals)>=10:\n",
    "            _av = np.average(stest.pvals)\n",
    "            if _av > 0.59:\n",
    "                mt6.append((sid, _av, len(stest.pvals), stest.pvals))\n",
    "\n",
    "for x in mt6:\n",
    "    print(x)\n",
    "#json.dump(js, open('syso-susp-pvals.json', 'w+'), indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(js, open('syso-susp-pvals.json', 'w+'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "rv = uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(uniform.ppf(0.01),uniform.ppf(0.99), 100)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, uniform.pdf(x), 'r-', lw=5, alpha=0.6, label='uniform pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
